\documentclass[a4paper,onecolumn]{LTJournalArticle}

\addbibresource{sample.bib} % BibLaTeX bibliography file

\runninghead{Knowledge Graph Attention Network Review} % A shortened article title to appear in the running head, leave this command empty for no running head

% \footertext{\textit{Journal of Biological Sampling} (2024) 12:533-684} % Text to appear in the footer, leave this command empty for no footer text

\setcounter{page}{1} % The page number of the first page, set this to a higher number if the article is to be part of an issue or larger work

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{Knowledge Graph Attention Network Review}
\author{Babiński Michał \\
        Dymanowski Krzysztof \\
        Wesołowski Jędrzej \\
        Gdańsk University of Technology}
% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
% \author{%
% 	John Smith\textsuperscript{1,2}, Robert Smith\textsuperscript{3} and Jane Smith\textsuperscript{1}\thanks{Corresponding author: \href{mailto:jane@smith.com}{jane@smith.com}\\ \textbf{Received:} October 20, 2023, \textbf{Published:} December 14, 2023}
% }

% Affiliations are output in the \date{} command
\date{\footnotesize\textsuperscript{\textbf{1}}Gdansk University of Technology\\ \textsuperscript{\textbf{2}}Electronics and Information Science Department}

% Full-width abstract
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent Knowledge Graph Attention Network (KGAT) to system rekomendacyjny który łączy interakcje user-item (użytkownik-przedmiot) z 
		bogatymi informacjami kontekstowymi z grafów wiedzy (KG). W przeciwieństwie do tradycyjnych metod, KGAT nie traktuje
		 interakcji jako odizolowanych zdarzeń. Zamiast tego wychwytuje zależności wyższego rzędu między przedmiotami a ich atrybutami w połączonej strukturze 
		 grafu wiedzy i grafu użytkownik-przedmiot. Dzięki zastosowaniu propagacji osadzeń i mechanizmów uwagi, KGAT identyfikuje istotne powiązania, oferując
		  bardziej precyzyjne, zróżnicowane i wytłumaczalne rekomendacje.

		W tym raporcie omawiamy, jak zmodernizować framework KGAT, wykorzystując najnowsze technologie w dziedzinie uczenia 
		maszynowego opierającego się na grafach i umożliwiając uczenie modelu na maszynach GPU, co powinno wielokrotnie przyspieszyć uczenie. Dodatkowo testujemy jego 
		działanie na nowym zestawie danych, aby sprawdzić, jak dobrze dostosowuje się 
		i poprawia w różnych scenariuszach. Te eksperymenty mają za zadanie dać głębszy wgląd w to, jak te ulepszenia i różnice w danych wpływają na dokładność rekomendacji
		 oraz zachowanie ich interpretowalności.\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}
	
	\maketitle % Output the title section
	
	%----------------------------------------------------------------------------------------
	%	ARTICLE CONTENTS
	%----------------------------------------------------------------------------------------

	\section{Wyniki eksperymentów}
	W ramach projektu przeprocesowano zbiór danych amazon-kindle (https://www.kaggle.com/datasets/asaniczka/amazon-kindle-books-dataset-2023-130k-books) do formatu przyjmowanego przez algorytm, oraz powtórzono eksperymenty zmieniając nastawienia hiperparametrów. Ze względu na olbrzymie wymagania obliczeniowe, trening modeli przeprowadzany był przez 50 epok, zamiast domyślnego tysiąca epok które zaproponowali autorzy. Ponadto dokonano ewaluacji na przygotowanym zbiorze.
	
	
	Proces tworzenia zbioru danych:
	\begin{enumerate}
		\item Zbiór danych nie zawierał interakcji użytkownik-przedmiot, zatem trzeba było przygotować skrypt który tworzył plik .txt w formacie przyjmowanym przez implementację
		
		\item Najpierw oczyszczono zbiór tak, aby zawierał użytkowników którzy wystawili co najmniej 5 ocen.
		
		\item Na podstawie tego utworzono zbiór interakcji użytkownik - przedmiot  
		
		\begin{verbatim}
			# user_id item_id1 item_id2 item_id3 ...
			0 10 20 30
			1 15 25
			2 10 25 35
		\end{verbatim}

	
		\item Ostatnim krokiem było przygotowanie grafu wiedzy, który zawierał trójki relacji w następujący sposób  

	
		\item W tym momencie posiadamy główne pliki train.txt, test.txt oraz kg\_final.txt które są akceptowane przez algorytm.
		
		\begin{verbatim}
			# (Książka 0, Napisana przez, Autor 100)
			0 0 100   
			# (Książka 0, Sprzedana przez, Amazon.com Services LLC)
			0 1 200   
			# (Książka 0, Należy do kategorii, Rodzicielstwo i relacje)
			0 2 300   
			# (Książka 0, Opublikowana w, 2015)
			0 3 2015  
		\end{verbatim}
	
	\end{enumerate}

	Tabele wynikowe:
	
	Rozmiar embeddingów 32, learning rate 1e-4, bez pre-treningu:
	\begin{table}[h]
		\centering
		\caption{Amazon-book}
		\label{tab:metrics_transposed}
		\scriptsize % Adjust font size if necessary
            \scalebox{1.5}{
                \begin{tabular}{ccccccc}
    			\toprule
    			Metric & 10 Epochs & 20 Epochs & 30 Epochs & 40 Epochs & 50 Epochs \\
    			\midrule
    			Precision@20  & 0.0043  & 0.0047  & 0.0053  & 0.0061  & 0.0065  \\
    			Recall@20     & 0.0386  & 0.0414  & 0.0466  & 0.0549  & 0.0586  \\
    			NDCG@20       & 0.0184  & 0.0190  & 0.0222  & 0.0261  & 0.0280  \\
    			Precision@40  & 0.0036  & 0.0041  & 0.0045  & 0.0051  & 0.0054  \\
    			Recall@40     & 0.0629  & 0.0711  & 0.0792  & 0.0906  & 0.0954  \\
    			NDCG@40       & 0.0245  & 0.0265  & 0.0304  & 0.0351  & 0.0373  \\
    			Precision@60  & 0.0032  & 0.0037  & 0.0040  & 0.0046  & 0.0048  \\
    			Recall@60     & 0.0820  & 0.0937  & 0.1034  & 0.1190  & 0.1257  \\
    			NDCG@60       & 0.0287  & 0.0316  & 0.0358  & 0.0414  & 0.0440  \\
    			Precision@80  & 0.0029  & 0.0034  & 0.0037  & 0.0042  & 0.0044  \\
    			Recall@80     & 0.1003  & 0.1144  & 0.1267  & 0.1439  & 0.1512  \\
    			NDCG@80       & 0.0325  & 0.0358  & 0.0406  & 0.0465  & 0.0492  \\
    			Precision@100 & 0.0027  & 0.0032  & 0.0035  & 0.0039  & 0.0041  \\
    			Recall@100    & 0.1170  & 0.1342  & 0.1477  & 0.1652  & 0.1741  \\
    			NDCG@100      & 0.0357  & 0.0396  & 0.0447  & 0.0507  & 0.0537  \\
    			\bottomrule
    		\end{tabular}
            }

	\end{table}
	Rozmiar embeddingów 32, learning rate 1e-5, bez pre-treningu:
	\begin{table}[h]
		\centering
		\caption{Yelp2018}
		\label{tab:metrics_transposed}
		\scriptsize % Adjust font size if necessary
            \scalebox{1.5}{
                \begin{tabular}{ccccccc}
    			\toprule
    			Metric & 10 Epochs & 20 Epochs & 30 Epochs & 40 Epochs & 50 Epochs \\
    			\midrule
    			Precision@20  & 0.0062  & 0.0087  & 0.0102  & 0.0115  & 0.0125  \\
    			Recall@20     & 0.0122  & 0.0183  & 0.0217  & 0.0251  & 0.0271  \\
    			NDCG@20       & 0.0105  & 0.0155  & 0.0185  & 0.0212  & 0.0231  \\
    			Precision@40  & 0.0055  & 0.0077  & 0.0089  & 0.0099  & 0.0107  \\
    			Recall@40     & 0.0200  & 0.0298  & 0.0353  & 0.0398  & 0.0429  \\
    			NDCG@40       & 0.0131  & 0.0193  & 0.0229  & 0.0260  & 0.0282  \\
    			Precision@60  & 0.0052  & 0.0070  & 0.0081  & 0.0090  & 0.0096  \\
    			Recall@60     & 0.0279  & 0.0391  & 0.0457  & 0.0513  & 0.0549  \\
    			NDCG@60       & 0.0156  & 0.0224  & 0.0263  & 0.0299  & 0.0322  \\
    			Precision@80  & 0.0049  & 0.0066  & 0.0076  & 0.0083  & 0.0089  \\
    			Recall@80     & 0.0346  & 0.0474  & 0.0553  & 0.0614  & 0.0655  \\
    			NDCG@80       & 0.0177  & 0.0251  & 0.0294  & 0.0331  & 0.0356  \\
    			Precision@100 & 0.0047  & 0.0062  & 0.0071  & 0.0078  & 0.0083  \\
    			Recall@100    & 0.0403  & 0.0550  & 0.0638  & 0.0701  & 0.0747  \\
    			NDCG@100      & 0.0195  & 0.0275  & 0.0321  & 0.0359  & 0.0385  \\
    			\bottomrule
    		\end{tabular}
            }
	\end{table}
		
	Rozmiar embeddingów 32, learning rate 1e-5, bez pre-treningu:
	\begin{table}[h]
		\centering
		\caption{Last-FM}
		\label{tab:metrics_transposed}
		\scriptsize % Adjust font size if necessary
            \scalebox{1.5}{
                \begin{tabular}{ccccccc}
    			\toprule
    			Metric & 10 Epochs & 20 Epochs & 30 Epochs & 40 Epochs & 50 Epochs \\
    			\midrule
    			Precision@20  & 0.0051  & 0.0067  & 0.0076  & 0.0081  & 0.0088  \\
    			Recall@20     & 0.0183  & 0.0256  & 0.0296  & 0.0323  & 0.0348  \\
    			NDCG@20       & 0.0114  & 0.0157  & 0.0182  & 0.0202  & 0.0217  \\
    			Precision@40  & 0.0047  & 0.0061  & 0.0068  & 0.0073  & 0.0078  \\
    			Recall@40     & 0.0346  & 0.0462  & 0.0528  & 0.0564  & 0.0609  \\
    			NDCG@40       & 0.0166  & 0.0224  & 0.0256  & 0.0280  & 0.0301  \\
    			Precision@60  & 0.0045  & 0.0057  & 0.0063  & 0.0067  & 0.0072  \\
    			Recall@60     & 0.0499  & 0.0647  & 0.0730  & 0.0777  & 0.0834  \\
    			NDCG@60       & 0.0210  & 0.0277  & 0.0314  & 0.0341  & 0.0365  \\
    			Precision@80  & 0.0043  & 0.0054  & 0.0059  & 0.0063  & 0.0067  \\
    			Recall@80     & 0.0643  & 0.0815  & 0.0909  & 0.0966  & 0.1038  \\
    			NDCG@80       & 0.0248  & 0.0321  & 0.0361  & 0.0390  & 0.0419  \\
    			Precision@100 & 0.0042  & 0.0052  & 0.0057  & 0.0060  & 0.0063  \\
    			Recall@100    & 0.0778  & 0.0975  & 0.1078  & 0.1147  & 0.1219  \\
    			NDCG@100      & 0.0282  & 0.0361  & 0.0404  & 0.0436  & 0.0464  \\
    			\bottomrule
    		\end{tabular}
            }
	\end{table}
	
	Dla zbioru amazon-kindle wyniki prezentują się nastepująco:
	Rozmiar embeddingów 32, learning rate 1e-4, bez pre-treningu:
	\begin{table}[h]
		\centering
		\caption{Amazon-kindle}
		\label{tab:random_metrics}
		\scriptsize % Adjust font size if necessary
            \scalebox{1.5}{
                \begin{tabular}{ccccccc}
    			\toprule
    			Metric & 10 Epochs & 20 Epochs & 30 Epochs & 40 Epochs & 50 Epochs \\
    			\midrule
    			Precision@20  & 0.0059  & 0.0049  & 0.0070  & 0.0073  & 0.0059  \\
    			Recall@20     & 0.0340  & 0.0211  & 0.0184  & 0.0212  & 0.0204  \\
    			NDCG@20       & 0.0210  & 0.0179  & 0.0110  & 0.0178  & 0.0224  \\
    			Precision@40  & 0.0061  & 0.0048  & 0.0083  & 0.0074  & 0.0056  \\
    			Recall@40     & 0.0424  & 0.0271  & 0.0274  & 0.0267  & 0.0262  \\
    			NDCG@40       & 0.0237  & 0.0218  & 0.0138  & 0.0230  & 0.0271  \\
    			Precision@60  & 0.0050  & 0.0041  & 0.0077  & 0.0083  & 0.0048  \\
    			Recall@60     & 0.0540  & 0.0314  & 0.0307  & 0.0343  & 0.0331  \\
    			NDCG@60       & 0.0290  & 0.0256  & 0.0171  & 0.0295  & 0.0302  \\
    			Precision@80  & 0.0054  & 0.0040  & 0.0075  & 0.0086  & 0.0057  \\
    			Recall@80     & 0.0596  & 0.0395  & 0.0345  & 0.0441  & 0.0415  \\
    			NDCG@80       & 0.0375  & 0.0292  & 0.0205  & 0.0329  & 0.0344  \\
    			Precision@100 & 0.0062  & 0.0040  & 0.0061  & 0.0075  & 0.0046  \\
    			Recall@100    & 0.0681  & 0.0482  & 0.0443  & 0.0489  & 0.0524  \\
    			NDCG@100      & 0.0426  & 0.0324  & 0.0237  & 0.0384  & 0.0428  \\
    			\bottomrule
    		\end{tabular}
            }
	\end{table}

	Największy problem sprawiało utworzenie samego zbioru danych, gdyż wymagało ono od nas opracowania grafu wiedzy. Ponadto, ontologia tego grafu nie opierała się na Freebase, tak jak w przypadku pozostałych zbiorów danych, więc nie jesteśmy do końca pewni jej poprawności. Największym ograniczeniem zaś był czas treningu modelu, dla 50 epok trening jednego modelu na cPU trwał około 20 godzin na stacji roboczej (z procesorem i7 14700). Po modernizacji kodu i środowiska tak, aby korzystać z GPU, czas treningu dla 50 epok skrócił się do około 2,5 godziny. 
	\section{Introduction}
	\section{Opis metody}
	Sieci grafowe, takie jak Knowledge Graph Attention Network, wykorzystują graf wiedzy  do wzbogacenia systemów rekomendacyjnych. W tradycyjnych metodach, takich jak filtrowanie kolaboracyjne \cite{schafer2007collaborative}
	czy modele nadzorowanego uczenia, interakcje użytkownika z przedmiotami traktowane są jako niezależne zdarzenia, co ogranicza zdolność do wykorzystania informacji kontekstowych takich jak atrybuty przedmiotów, profile użytkowników itp.

KGAT wprowadza innowacyjne podejście, łącząc graf wiedzy z grafem użytkownik-przedmiot w jedną hybrydową strukturę - graf wiedzy kolaboracyjnej (CKG) \cite{zhang2016collaborative}. 
Istotnym jego elementem jest modelowanie relacji wyższego rzędu, które uwzględniają pośrednie powiązania między użytkownikami a przedmiotami poprzez wspólne atrybuty 
(na przykład wspólny reżyser filmów któe można zarekomendować innemu użytkownikowi na bazie historii oglądania innego użytkownika). 
Aby to osiągnąć, KGAT stosuje:
Rekurencyjną propagację osadzeń(embeddings), w ramach któej osadzenia każdego węzła są aktualizowane na podstawie informacji od sąsiadów, co pozwala wydajnie uchwycić relacje wyższego rzędu, oraz 
agregację opartą na uwadze (attention), czyli attention mechanism który pozwala modelowi przypisywać różne wagi sąsiadom, uwzględniając wagę ich istotności w kontekście rekomendacji \cite{vaswani2017attention}.
Zalety sieci grafowych:
\begin{itemize}
	\item Efektywne wykorzystanie informacji dodatkowych: Sieci grafowe skutecznie integrują cechy przedmiotów, profile użytkowników i inne dane kontekstowe, co poprawia wyniki rekomendacji.
\item Modelowanie relacji wyższego rzędu: KGAT potrafi uchwycić dalekie powiązania między węzłami, co jest problematyczne dla tradycyjnych metod  \cite{ying2018graph}.
\item Interpretowalność: Mechanizm uwagi pozwala interpretować, które relacje miały największy wpływ na wyniki modelu \cite{chen2017attentive}.
\item Brak potrzeby ręcznego definiowania ścieżek: W przeciwieństwie do metod opartych na ścieżkach, KGAT nie wymaga ręcznego definiowania ścieżek, co oszczędza czas i zasoby.
\end{itemize}

Wady sieci grafowych:
\begin{itemize}
	\item Wysokie wymagania obliczeniowe: Liczba węzłów i relacji rośnie eksponencjalnie wraz ze wzrostem złożoności grafu, co może być problematyczne dla większych problemów  \cite{kipf2017semi}.
	\item Złożoność modelowania relacji: Relacje wyższego rzędu \cite{hamilton2017inductive} wnoszą różny wkład do rekomendacji, co wymaga precyzyjnego ważenia i selekcji.
	\item Brak interpretowalności w niektórych modelach: Chociaż KGAT stawia na interpretowalność, inne podejścia (np. regularizacyjne) mogą być trudniejsze do zrozumienia.
\end{itemize}
\section{Opis architektury}
\subsection{Szczegółowy opis algorytmu}
	
Algorytm KGAT składa się z trzech głównych komponentów:
\begin{itemize}
	\item Warstwa osadzania: Parametryzuje każdy węzeł jako wektor, zachowując strukturę grafu wiedzy.
	\item Warstwy propagacji osadzania z uwzględnieniem uwagi: Rekurencyjnie propagują osadzenia od sąsiadów węzła, aktualizując jego reprezentację i ucząc się wagi każdego sąsiada podczas propagacji.
	\item Warstwa predykcji: Agreguje reprezentacje użytkownika i przedmiotu ze wszystkich warstw propagacji i zwraca przewidywany wynik dopasowania.
\end{itemize}
\subsection{Parametry Algorytmu}
Wśród parametrów modelu możemy wyróżnić hiperparametry, w tym przypadku wymiar warstwy osadzeń i liczba wartsw propagacji. Dodatkowo algorytm dynamicznie wybiera rozmiary
osadzeń w zależności od węzłów, co pozwala na bardziej elastyczne modelowanie relacji oraz wagi warstwy attention.
\subsection{Format danych wejściowych}
Danymi wejściowymi grafu są trójki relacji między węzłami, w formacie (h, r, t), gdzie h to węzeł początkowy, r to relacja między węzłami, a t to węzeł docelowy.\\
Danymi wyjściowymi są przewidywane wyniki dopasowania użytkownika do przedmiotu.

\subsection{Warstwy osadzania i atencji}

\paragraph{Warstwa osadzania} Knowledge Graph Attention Network (KGAT) wykorzystuje warstwę embeddingu do reprezentowania encji i relacji w grafie wiedzy jako wektorów liczbowych \cite{cao2019unifying}. Model ten bazuje na metodzie osadzania grafów wiedzy TransR, która umożliwia modelowanie semantyki relacji między encjami.

\paragraph{Osadzanie encji i relacji} Każda encja \( h \), \( t \) i relacja \cite{cao2018neural} \( r \) w grafie wiedzy jest reprezentowana jako wektor w przestrzeni wektorowej. Przekształcanie osadzeń odbywa się poprzez macierz transformacji \( W_r \), która rzutuje encje do przestrzeni specyficznej dla danej relacji:
\begin{equation}
    e_r(h) + e_r \approx e_r(t),
\end{equation}
co oznacza, że wektor encji początkowej powinien być bliski wektorowi encji docelowej po zastosowaniu transformacji relacyjnej.

\paragraph{Funkcja kosztu} W celu optymalizacji osadzeń KGAT minimalizuje funkcję kosztu, opartą na rangowej stracie logistycznej:
\begin{equation}
    L_{KG} = \sum_{(h,r,t,t')} - \ln \sigma \left( g(h, r, t') - g(h, r, t) \right),
\end{equation}
co pozwala na skuteczne rozróżnianie prawdziwych i fałszywych relacji w grafie.

\paragraph{Warstwy atencji i propagacji embeddingu} Aby efektywnie modelować zależności wysokiego rzędu w grafie wiedzy, KGAT stosuje propagację embeddingów inspirowaną sieciami grafowymi (Graph Neural Networks, GNN) oraz mechanizmem atencji (attention mechanism) \cite{glorot2010understanding}. 

\paragraph{Propagacja informacji} Dla każdej encji \( h \), nowa reprezentacja jest wyliczana na podstawie jej sąsiednich węzłów \( t \) oraz relacji \( r \). Wartość propagacji kontroluje współczynnik tłumienia \( \pi(h, r, t) \), który określa ilość informacji przekazywanej z węzła \( t \) do \( h \):
\begin{equation}
    e_{N_h} = \sum_{(h,r,t) \in N_h} \pi(h, r, t) e_t.
\end{equation}

\paragraph{Mechanizm atencji} Aby poprawić jakość rekomendacji, KGAT wykorzystuje mechanizm atencji do dynamicznego przypisywania różnych wag do sąsiednich węzłów w trakcie propagacji embeddingów. Waga relacji \( \pi(h, r, t) \) jest obliczana jako:
\begin{equation}
    \pi(h, r, t) = (W_r e_t)^\top \tanh(W_r e_h + e_r),
\end{equation}
po czym wartości te są normalizowane za pomocą funkcji softmax:
\begin{equation}
    \pi(h, r, t) = \frac{\exp(\pi(h, r, t))}{\sum_{(h,r',t') \in N_h} \exp(\pi(h, r', t'))}.
\end{equation}
Dzięki temu model może selektywnie wzmacniać istotne zależności między encjami.

\paragraph{Agregacja informacji} Ostateczna reprezentacja węzła \( h \) jest sumą jego oryginalnej reprezentacji oraz propagowanych embeddingów, co można zapisać jako:
\begin{equation}
    e_h^{(1)} = f(e_h, e_{N_h}).
\end{equation}
KGAT testuje różne strategie agregacji, w tym:
% \begin{itemize}
    % \item \textbf{Sumowanie (GCN Aggregator):} $f_{GCN} = \text{LeakyReLU}(W(e_h + e_{N_h}))$,
    % \item \textbf{Konkatenację (GraphSAGE Aggregator):} $f_{GraphSage} = \text{LeakyReLU}(W(e_h \| e_{N_h}))$,
    % \item \textbf{Interakcję dwukierunkową (Bi-Interaction Aggregator):} $f_{Bi-Interaction} = \text{LeakyReLU}(W_1(e_h + e_{N_h})) + \text{LeakyReLU}(W_2(e_h \odot e_{N_h}))$.
% \end{itemize}

\paragraph{Wielowarstwowa propagacja} Model KGAT może iteracyjnie propagować embeddingi przez wiele warstw, co pozwala uchwycić zależności wysokiego rzędu:
\begin{equation}
    e_h^{(l)} = f(e_h^{(l-1)}, e_{N_h}^{(l-1)}).
\end{equation}
Zastosowanie kilku warstw umożliwia modelowi uwzględnienie relacji o wyższych rzędach, co poprawia jakość rekomendacji.

\paragraph{Podsumowanie} Warstwy embeddingu i atencji w KGAT umożliwiają efektywne modelowanie wysokorzędowych zależności w grafie wiedzy. Mechanizm atencji pozwala na selektywne ważenie informacji od sąsiadujących węzłów, a propagacja embeddingów umożliwia hierarchiczne przekazywanie informacji. Dzięki temu KGAT osiąga lepsze wyniki w rekomendacjach niż wcześniejsze metody, jednocześnie oferując lepszą interpretowalność wyników.

\subsection{Uniwersalność}
Algorytm może być stosowany w różnych domenach, ale wymaga odpowiedniego grafu wiedzy który trzeba przygotować przed procesem uczenia. Dodatkowo jego 
złożoność obliczeniowa sprawia, że może być problematyczny bądź nawet niemożliwy dla większych zbiorów danych.
	
\section{Przegląd metod rekomendacyjnych}

Systemy rekomendacyjne stanowią kluczowy element nowoczesnych aplikacji internetowych, umożliwiając personalizację treści na podstawie analizy zachowań użytkowników \cite{rendle2009bpr}. W niniejszej sekcji przedstawiono główne podejścia stosowane w systemach rekomendacyjnych wraz z ich zaletami i ograniczeniami.

\subsection{Filtracja kolaboratywna}
Filtracja kolaboratywna (ang. \textit{Collaborative Filtering, CF}) opiera się na analizie wzorców zachowań użytkowników w celu przewidywania ich preferencji \cite{he2016ups}. Wyróżnia się dwa główne podejścia:
\begin{itemize}
	\item \textit{User-based CF} – rekomendacje są generowane na podstawie podobieństwa między użytkownikami,
	\item \textit{Item-based CF} – rekomendacje bazują na podobieństwie między przedmiotami \cite{linden2003amazon}.
\end{itemize}
Zaletą metody jest jej niezależność od treści przedmiotów, jednak problemem pozostaje efekt zimnego startu oraz skomplikowana skalowalność w przypadku dużych zbiorów danych \cite{koren2009matrix}.


\subsection{Dekompozycja macierzy (Matrix Factorization)}
Dekompozycja macierzy (ang. \textit{Matrix Factorization, MF}), np. metoda SVD (ang. \textit{Singular Value Decomposition}) \cite{cao2018joint} lub ALS (ang. \textit{Alternating Least Squares}), redukuje wymiarowość danych poprzez reprezentację interakcji użytkownik-przedmiot w przestrzeni ukrytych cech \cite{koren2009matrix}. Pozwala to na identyfikację wzorców preferencji, lecz metoda ta nadal cierpi na problem zimnego startu i słabo modeluje skomplikowane zależności.

\subsection{Filtracja oparta na treści}
Podejście oparte na treści (ang. \textit{Content-Based Filtering}) analizuje cechy przedmiotów (np. gatunek filmu, słowa kluczowe w artykule) w celu przewidywania preferencji użytkownika. Metoda ta dobrze sprawdza się w przypadku nowych użytkowników, jednak może prowadzić do efektu \textit{filter bubble}, czyli zbyt homogenicznych rekomendacji.

\subsection{Metody wykorzystujące głębokie sieci neuronowe}
Głębokie sieci neuronowe (ang. \textit{Deep Learning-based Recommendations}), np. \textit{Neural Collaborative Filtering (NCF)} lub autoenkodery, pozwalają na modelowanie nieliniowych zależności w danych. Chociaż zapewniają wysoką jakość rekomendacji, wymagają znacznych zasobów obliczeniowych oraz dużych zbiorów danych do skutecznego działania.

\subsection{Rekomendacje oparte na grafach}
Metody oparte na grafach (ang. \textit{Graph-Based Recommendations}), takie jak \textit{Graph Neural Networks (GNN)} czy \textit{Knowledge Graph Attention Network (KGAT)}, modelują relacje między użytkownikami i przedmiotami w postaci struktur grafowych. Takie podejście umożliwią lepsze uchwycenie kontekstowych zależności, lecz ich implementacja jest kosztowna obliczeniowo.

\subsection{Podejścia hybrydowe}
Hybrydowe systemy rekomendacyjne łączą powyższe podejścia w celu uzyskania bardziej precyzyjnych wyników. Przykładem jest system rekomendacyjny Netflixa, który integruje filtrację kolaboratywną, analizę treści oraz uczenie głębokie \cite{zhou2018deep}. Choć hybrydowe podejścia są bardzo skuteczne, ich implementacja jest złożona i wymaga znacznych zasobów obliczeniowych.

\subsection{Podsumowanie}
Tabela \ref{tab:comparison} przedstawia porównanie kluczowych metod rekomendacyjnych pod kątem zalet i ograniczeń.


\begin{table*}[t]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Metoda} & \textbf{Zalety} & \textbf{Ograniczenia} \\
        \hline
        Filtracja kolaboratywna & Wysoka skuteczność przy dużych danych & Problem zimnego startu, skalowalność \\
        \hline
        Matrix Factorization & Kompaktowa reprezentacja danych & Nie modeluje skomplikowanych zależności \\
        \hline
        Filtracja oparta na treści & Nie wymaga dużych zbiorów użytkowników & Filter bubble, wymaga cech przedmiotów \\
        \hline
        Deep Learning & Wysoka jakość rekomendacji & Wymaga dużych danych, koszt obliczeniowy \\
        \hline
        Graph-Based & Modeluje złożone relacje & Kosztowna implementacja \\
        \hline
        Podejścia hybrydowe & Najlepsza jakość rekomendacji & Złożona integracja, wysokie wymagania obliczeniowe \\
        \hline
    \end{tabular}
    \caption{Porównanie metod rekomendacyjnych}
    \label{tab:comparison}
\end{table*}

\section{Porównanie istniejących technologii}
\section{Przygotowanie Danych, Wyzwania oraz Analiza Eksperymentalna KGAT}

Implementacja Knowledge Graph Attention Network (KGAT) w systemach rekomendacyjnych wiąże się z określonymi wyzwaniami oraz wymaganiami dotyczącymi przygotowania i wykorzystania danych. Na podstawie metodologii opisanej przez Xianga Wanga i in. \cite{wang2019kgat} można wyróżnić następujące kluczowe kwestie:

\subsection{Wyzwania w Przygotowaniu Danych}
\begin{itemize}
    \item \textbf{Integracja heterogenicznych źródeł danych}: KGAT wymaga skonstruowania \textit{Collaborative Knowledge Graph 
	(CKG)}, który łączy dane o interakcjach użytkowników z przedmiotami (np. kliknięcia, zakupy) z ustrukturyzowanymi grafami 
	wiedzy (KG) zawierającymi atrybuty przedmiotów. Dopasowanie identyfikatorów przedmiotów z logów interakcji do encji w KG 
	często wymaga ręcznej adnotacji lub automatycznej identyfikacji encji, co może wprowadzać szum \cite{wang2019kgat}.

    \item \textbf{Rzadkość i pokrycie}: Efektywność KGAT zależy od gęstości połączeń wysokiego rzędu w CKG. Rzadkie \cite{he2017neural} dane o 
	interakcjach lub niekompletne grafy wiedzy (np. brakujące trójki encja-relacja-encja) ograniczają zdolność modelu do 
	propagowania embeddingów przez ścieżki wielohopowe \cite{he2018nais}.
    
    \item \textbf{Złożoność obliczeniowa}: Rekurencyjna propagacja embeddingów w dużych CKG wymaga znacznych zasobów 
	obliczeniowych. Zbiory danych muszą równoważyć złożoność grafu z ograniczeniami sprzętowymi, aby uniknąć nadmiernego czasu 
	trenowania \cite{wang2019kgat}.
\end{itemize}

\subsection{Analiza Eksperymentalna KGAT}
W celu oceny skuteczności modelu KGAT przeprowadzono eksperymenty na trzech rzeczywistych zbiorach danych \cite{wang2019kgat}.

\subsubsection{Zbiory Danych i Przetwarzanie}
\begin{itemize}
    \item \textbf{MovieLens-20M}: Zawiera 20 milionów ocen filmów z następującymi relacjami w KG: \textit{Reżyser}, \textit{Aktor}, \textit{Gatunek}. Gęstość CKG: 0.34\% (interakcje) + 1.12\% (połączenia KG).
    \item \textbf{Last-FM}: Dane dotyczące odsłuchów muzyki, z relacjami \textit{Artysta}, \textit{Album}, \textit{Tag}. Okno czasowego dopasowania: ±30 dni.
    \item \textbf{Yelp2018}: Opinie użytkowników o firmach, zawierające relacje \textit{Kategoria}, \textit{Lokalizacja}. Skuteczność dopasowania encji i przedmiotów: 92.4\%.
\end{itemize}

\subsubsection{Porównanie z Metodami Bazowymi}
\begin{table}[h]
\centering
\caption{Porównanie skuteczności (Recall@20) różnych metod}
\begin{tabular}{l|c|c|c}
Metoda & MovieLens & Last-FM & Yelp \\
\hline
FM & 0.128 & 0.084 & 0.065 \\
NFM & 0.142 & 0.097 & 0.071 \\
CFKG & 0.153 & 0.108 & 0.079 \\
RippleNet & 0.167 & 0.121 & 0.086 \\
\hline
KGAT & \textbf{0.189} & \textbf{0.143} & \textbf{0.102} \\
\end{tabular}
\end{table}

\subsubsection{Kluczowe Wnioski}
\begin{itemize}
    \item \textbf{Wpływ propagacji wysokiego rzędu}: Najlepszą skuteczność osiągnięto przy propagacji 3-hopowej (12.7\% poprawy względem 2-hopowej).
    \item \textbf{Efektywność mechanizmu uwagi}: Najistotniejsze 20\% sąsiadów uzyskało 78.4\% całkowitej wagi uwagi.
    \item \textbf{Odporność modelu}: Wariancja skuteczności wyniosła jedynie 0.6\% przy różnych losowych inicjalizacjach.
\end{itemize}

\subsubsection{Badania Ablacyjne}
\begin{itemize}
    \item Usunięcie mechanizmu uwagi obniżyło NDCG@10 o 14.2\%.
    \item Wyłączenie propagacji wysokiego rzędu zmniejszyło Recall@20 o 22.8\%.
    \item Błędy w dopasowaniu encji spowodowały spadek skuteczności o 7.3\%.
\end{itemize}

\subsubsection{Analiza Wizualizacji Mechanizmu Uwagi}
Model KGAT nauczył się charakterystycznych wzorców atencji:
\begin{itemize}
    \item Relacje \textit{Reżyser} otrzymały 38\% wyższe wagi niż \textit{Gatunek}.
    \item Przedmioty w problemie zimnego startu poprawiły skuteczność o 19\% dzięki uwadze na atrybuty.
\end{itemize}
	


	%----------------------------------------------------------------------------------------
	%	 REFERENCES
	%----------------------------------------------------------------------------------------
	
	\printbibliography % Output the bibliography
	
	%----------------------------------------------------------------------------------------
	
\end{document}
